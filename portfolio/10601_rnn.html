<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
		<meta name="description" content="">
		<meta name="keywords" content="">
		<title>Project - Decision Tree</title>
		<!-- Favicon -->
		<link href="../assets/images/favicon.png" rel="shortcut icon">
		<!-- CSS -->
		<link href="../assets/plugins/bootstrap/bootstrap.min.css" rel="stylesheet">
		<link href="../assets/plugins/magnific-popup/magnific-popup.min.css" rel="stylesheet">
		<link href="../assets/plugins/swiper/swiper-bundle.min.css" rel="stylesheet">
		<link href="../assets/css/theme.css" rel="stylesheet">
		<!-- Fonts/Icons -->
		<link href="../assets/plugins/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
		<link href="../assets/plugins/fontawesome/css/all.css" rel="stylesheet">
	</head>
	<body data-preloader="true">

		<!-- Toggle Menu -->
		<div class="toggle-menu">
			<!-- Close button -->
			<button class="toggle-close">
				<i class="bi bi-x"></i>
			</button>
			<!-- end Close button -->
			<h6 class="mono-heading fw-normal mb-2">Phone:</h6>
			<h4 class="fw-medium">+(886) 909 756 966</h4>
			<div class="mt-4">
				<h6 class="mono-heading fw-normal mb-2">Email:</h6>
				<h6 class="fw-medium">moneychien20639@gmail.com</h6>
				<ul class="list-inline-sm mt-3">
					<li>
						<a class="button-circle button-circle-sm" href="https://www.facebook.com/profile.php?id=100006726992151"><i class="bi bi-facebook"></i></a>
					</li>
					<li>
						<a class="button-circle button-circle-sm" href="https://www.instagram.com/arthurchien_/"><i class="bi bi-instagram"></i></a>
					</li>
					<li>
						<a class="button-circle button-circle-sm" href="https://www.linkedin.com/in/arthur-chien-24b376207/"><i class="bi bi-linkedin"></i></a>
					</li>
				</ul>
			</div>
			<div class="mt-4 mt-lg-5">
				<ul class="list-circle">
					<li>
						<a class="mono-heading link-decoration" href="https://www.youtube.com/watch?v=BSFNl4roGlI&ab_channel=Raindrop"><i class="bi bi-music-note-beamed"></i> Give it a try?</a>
					</li>
					<!-- <li><a class="mono-heading link-decoration" href="#">Link item</a></li> -->
				</ul>
			</div>
			<div class="tm-bottom">
				<p>&copy; 2024 Yu-Hang</p>
			</div>
		</div>
		<!-- end Toggle Menu -->

		<!-- Main container -->
		<div class="container">

			<!-- Header -->
			<div id="header">
				<div class="row">
					<div class="col-12 col-lg-4 col-xl-3 order-lg-2 text-end">
						<div class="d-inline-flex align-items-center">
							<!-- Social Links -->
							<ul class="list-inline d-inline-block mono-heading fw-medium">
								<li>
									<a class="link-decoration" href="resume.html">Resume</a>
								</li>
								<li>
									<a class="link-decoration" href="https://github.com/arthur900530">github</a>
								</li>
								<li>
									<a class="link-decoration" href="https://scholar.google.com/citations?user=lkPbczcAAAAJ&hl=zh-TW">Scholor</a>
								</li>
							</ul>
							<!-- Toggle Menu Button -->
							<button class="menu-dots">
								<span></span>
							</button>
							<!-- end Toggle Menu Button -->
						</div>
					</div><!-- end col -->
					<div class="col-12 col-lg-8 order-lg-1 col-xl-9">
						<div class="py-4">
							<h1 class="display-2 fw-semi-bold m-0">Arthur <span class="stroke-text">Chien</span></h1>
						</div>
					</div><!-- end col -->
				</div><!-- end row -->
			</div>
			<!-- end Header -->
			
			<!-- Main row -->
			<div class="row g-4 g-lg-5">

				<div class="col-12 col-lg-4 col-xl-3">
					<!-- Sections Navigation -->
					<div class="nav-wrapper">
						<div class="section-nav">
							<ul class="nav">
								<li class="nav-item">
									<a class="nav-link" href="../index.html">
										<span class="nav-link-desktop">Home</span>
										<span class="nav-link-mobile">H</span>
										<span class="nav-circle"></span>
									</a>
								</li>
								<li class="nav-item">
									<a class="nav-link active" href="#">
										<span class="nav-link-desktop">Portfolio</span>
										<span class="nav-link-mobile">P</span>
										<span class="nav-circle"></span>
									</a>
								</li>
							</ul>
						</div><!-- end section-nav -->
					</div><!-- end nav-wrapper -->
					<!-- end Sections Navigation -->
				</div><!-- end col -->

				<div class="col-12 col-lg-8 col-xl-9">
					<div class="sections-wrapper">
						<!-- Project content -->
						<div class="section-box">
							<div class="row g-4">
								<div class="col-12 col-xl-5">
									<h6 class="mono-heading mb-0">Course:</h6>
									<p>Introduction to Machine Learning (10-601)</p>
								</div>
								<div class="col-12 col-xl-3">
									<h6 class="mono-heading mb-0">Time Spent:</h6>
									<p>10 hours</p>
								</div>
								<div class="col-12 col-xl-4">
									<h6 class="mono-heading mb-0">Source Code:</h6>
									<a class="mono-heading mb-0" href="https://github.com/arthur900530">to github</a>
								</div>
							</div><!-- end row -->
							<div class="mt-4">
                                <h1>Recurrent Neural Network with Attention</h1>
                                <p>
                                    In this assignment, I implemented a Recurrent Neural Network (RNN) with a scaled dot-product self-attention mechanism to perform next-token prediction on the TinyStories dataset. This project deepened my understanding of sequential modeling and attention-based architectures.
                                </p>
                                <br>
                            
                                <h4 class="fw-medium">Model Definition:</h4>
                                <ul class="list-circle">
                                    <li><strong>Architecture:</strong> RNN with a single hidden layer, scaled dot-product self-attention, and a linear language modeling head.</li>
                                    <li><strong>Key Components:</strong> 
                                        <ul>
                                            <li>RNN Cell: Maintains temporal context through hidden states.</li>
                                            <li>Self-Attention Layer: Highlights relevant tokens in a sequence via query-key-value mechanism.</li>
                                            <li>Language Modeling Head: Outputs token probabilities from the attention-enhanced RNN output.</li>
                                        </ul>
                                    </li>
                                </ul>
                                <br>
                            
                                <h4 class="fw-medium">Tasks Accomplished:</h4>
                                <ul class="list-circle">
                                    <li><strong>Forward Propagation:</strong> Built a computation graph combining token embeddings, RNN hidden states, attention outputs, and a linear projection layer.</li>
                                    <li><strong>Training Loop:</strong> Used stochastic gradient descent to minimize cross-entropy loss over next-token predictions, with proper shifting of targets.</li>
                                    <li><strong>Evaluation:</strong> Performed validation at regular intervals to monitor generalization and convergence.</li>
                                </ul>
                                <br>
                            
                                <h4 class="fw-medium">Implementation Details:</h4>
                                <ul class="list-circle">
                                    <li><strong>Embedding Layer:</strong> Used <code>nn.Embedding</code> to represent tokens as dense vectors.</li>
                                    <li><strong>Hyperparameter Tuning:</strong> Tuned embedding size, hidden dimensions, and attention settings.</li>
                                    <li><strong>Attention Module:</strong> Computed scaled dot-product attention via Q/K/V projections and softmax-weighted aggregation.</li>
                                    <li><strong>Output Generation:</strong> Implemented both greedy decoding and temperature-based sampling for sequence generation.</li>
                                </ul>
                                <br>
                            
                                <h4 class="fw-medium">Empirical Evaluations:</h4>
                                <ul class="list-circle">
                                    <li>Tested effects of embedding/hidden dimensions on model accuracy.</li>
                                    <li>Studied performance vs. batch size and training sequence count.</li>
                                    <li>Explored sampling temperatures (0, 0.3, 0.8) for text diversity.</li>
                                </ul>
                                <br>
                            
                                <h4 class="fw-medium">Programming Techniques:</h4>
                                <ul class="list-circle">
                                    <li><strong>Vectorization:</strong> Leveraged PyTorch matrix ops for efficient forward/backward passes.</li>
                                    <li><strong>Modular Design:</strong> Separated RNN, attention, and decoder logic for readability and reuse.</li>
                                    <li><strong>Debugging Tools:</strong> Verified correctness using unit tests and mini datasets.</li>
                                </ul>
                                <br>
                            
                                <h4 class="fw-medium">Outputs and Results:</h4>
                                <ul class="list-circle">
                                    <li>Logged per-epoch training and validation losses for trend analysis.</li>
                                    <li>Generated coherent sample texts via sampling-based decoding strategies.</li>
                                </ul>
                                <br>
                            
                                <p>
                                    This project enhanced my understanding of deep learning for language modeling, including recurrent computation, attention mechanisms, and token-level prediction. It also improved my fluency with PyTorch and experimental design.
                                </p>
                            
                                <br>
                                <ul class="list-inline-pills mt-3">
                                    <li>Recurrent Neural Networks</li>
                                    <li>Self-Attention</li>
                                    <li>Language Modeling</li>
                                    <li>Cross-Entropy Loss</li>
                                    <li>Sequence Modeling</li>
                                    <li>PyTorch</li>
                                    <li>Token Embedding</li>
                                    <li>Text Generation</li>
                                </ul>
                            </div>                            
							
						</div><!-- end section-box -->
						<!-- end Project Content -->
					</div>
				</div><!-- end col -->
			</div><!-- end main row -->

		</div><!-- end main container -->

		<!-- Background Vertical lines -->
		<div class="bg-lines">
			<div class="bg-line-1"></div>
			<div class="bg-line-2"></div>
			<div class="bg-line-3"></div>
			<div class="bg-line-4"></div>
		</div>
		<!-- end Background Vertical lines -->

		<!-- JavaScripts -->
		<script src="../assets/plugins/jquery.min.js"></script>
		<script src="../assets/plugins/plugins.js"></script>
		<script src="../assets/js/functions.js"></script>
	</body>
</html>